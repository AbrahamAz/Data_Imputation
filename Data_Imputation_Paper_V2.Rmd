---
title: "Data Imputation using ISMI data (Draft version)"
author: "Kindly do not use for external distribution and reporting."
output: html_document
---
## {.tabset}
### Introduction, Context & Reason
<h4><b>Introduction:</b></h4>
In this paper, we will be discussing multiple *data imputation* strategies. This work is inspired by an intial study conducted by <b>Josh McCormick</b>. 
<br>
This study started after we noticed some missing information in the <b>ISMI</b> (IDPs Situation Monitoring Initiative) research cycle. Especially, information on IDP arrivals numbers from <b>known</b> sub-districts. 
<br>

<h4><b> Bit of context:</b></h4>
Over the last 5 months, slightly more than 90% of IDP arrivals were reported to have a known place of origin. This pattern has been similar since the beginning of the project. <br>

```{r echo=FALSE, message=FALSE, warning=FALSE}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
rm(list = ls())
library(ggplot2)
library(tidyverse)
data <- readxl::read_xlsx("data/unknown_LPD.xlsx")
data$Month <- factor(data$Month, levels = c("September-2021","October-2021","November-2021","December-2021","January-2022"))

ggplot(data)+
  geom_bar(aes(x=Month, y=`Total # of IDP HH arrivals`),stat ="identity")+
  geom_bar(aes(x=Month, y=`Max Cap # HH LPD`,fill = "#red"),stat="identity")+
  geom_bar(aes(x=Month, y=`Sum of # HH LPD`,fill = "#ee5859"),stat="identity")+
  geom_text(aes(x=Month, y=`Total # of IDP HH arrivals`,
                label= paste0(round(100-(100*(`Max Cap # HH LPD`/`Total # of IDP HH arrivals`)),2),"%"))
            ,nudge_y=  100, stat = "identity")+
  geom_text(aes(x=Month, y=`Sum of # HH LPD`
                ,label= paste0(round(100*(`Sum of # HH LPD`/`Total # of IDP HH arrivals`),2),"%")),
            nudge_y= - 100, stat = "identity")+
  geom_text(aes(x=Month, y=`Sum of # HH LPD`
                ,label= paste0(round(round((100-(100*(`Sum of # HH LPD`/`Total # of IDP HH arrivals`))),2)-round((100-(100*(`Max Cap # HH LPD`/`Total # of IDP HH arrivals`))),2),2),"%")),
              nudge_y= 80, stat = "identity")+
  theme(legend.position = "none")
```

<br><p style= "color: red;">*ISMI is collected on a bi-weekly basis, hence why the data used for the plot was only including the first half of January 2022*</p>
The above plot shows the number of IDP arrivals per month. Because of the limitations in the tool (asking a maximum of three places of departures), we have to calculate the sum of arrivals from the three mentioned places of departures and compare it to the total number of arrival to the community. Sometimes, the sum is still less then total number and doesn't give a place for Enumerator to report on other places of departures (assuming that they know). The ditribution of the data in the plot shows: <br>
<ul>
<li>In red, the  number of IDP arrivals from known sub-districts</li>
<li>In blue, the number of IDP arrivals with possibility of known sub-districts because of tool limitations</li>
<li>In grey, the number of IDP arrivals from unknown sub-districts</li>
</ul>

So as a conclusion, taking September 2021 as an example, the % of unknown sub-district of departure lie down somewhere between 2.54% and 7.76%.<br>


<h4><b>Reason:</b></h4>

The main reason we are doing this study is to fill the gap of unknown locations because this data is sent directly to CCCM, who use it for their monthly displacement tracking updates. However, they do not use the total number of IDP arrivals, they only look at the total IDP arrivals <b>from known sub-district only</b>. This might cause some people to assume the total number of arrivals is lower than is actually is, generating a false picture of displacement in the area of studies. <br>

The goal of this study is to try different algorithms of replacing the missing data and comparing them, and hopefully be able to implement the best methodology in other places/assessments when needed.

### Methodology

<h4><b>Methodologies used:</b></h4>

Two strategies were used to study data imputations in this context, the Hot deck and the kNN imputations. <br>

<h5><b>Hot deck:</b></h5>

Hot deck imputation is a method for handling missing data in which each missing value is replaced with an observed response from a “similar” unit.
Hot deck imputation involves replacing missing values of one or more variables for a non-respondent (called the recipient) with observed values from a respondent (the donor) that is similar to the non-respondent with respect to characteristics observed by both cases.
This theory is not as well developed as that of other imputation methods.<br>

References:<br>
[Hot Deck Methodology](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3130338/)

<h5><b>kNN or k-Nearest Neighbours:</b></h5>

kNN identifies the neighboring points through a measure of distance and the missing values can be estimated using completed values of neighbouring observations.
In another word, it identify 'k' samples in the dataset that are similar or close in space, and this sample is used to estimate values of the missing data points using the mean value of the 'k'-neighbours round in the dataset.<br>

References:<br>
[kNN Methodology](https://www.analyticsvidhya.com/blog/2020/07/knnimputer-a-robust-way-to-impute-missing-values-using-scikit-learn/#:~:text=The%20idea%20in%20kNN%20methods,neighbors%20found%20in%20the%20dataset.)

<h4><b>Methods for inserting NAs:</b></h4>

We also did use two methodologies to insert NAs in our ISMI dataset: <br>
<ul>
<li> Randomized selection of NA values depending on the percentage of missing data from previous rounds</li>
<li> Forced selection of NA values depending on the geographical distribution of NAs for the same round</li>
</ul>

For both, randomized and forced selection of NAs, we used a full dataset from ISMI (without NAs) and generated NAs entries in the *From Sub-district* field. 

<h4><b>Error rate:</b></h4>
For each *movement entry* *, we gave unique IDs to be able to recognize the data that was used for the imputation. Taking only the data that was imputed, we calculated the error rate by dividing the total number of HH of arrivals that resulted with a wrong imputation by the total number of HH of arrivals that was imputed.


<h4><b>Simulations:</b></h4>
Multiple simulations were run using different parameters and variables. <br>
The next two tabs show the same simulations off impuations but using either randomized NAs or forced NAs.<br>
Each plot in both tabs represent different level of imputation on which the error rate is calculated (community and household levels). All the plots represent the aggregation of the data on community level. <br>

<h4><b>*Some hints to understand reading labels in the:* </b></h4>

As there will be different simulations, using different methodologies, parameters, # of simulation, etc. I will be using a code for each simulation with different sections as following:<br>
<h5>*STRAT_LVLIMP_ORD_DOM* --> for hot deck strategy</h5>
*STRAT* = Strategy used<br>
*LVLIMP* = Level of imputation<br>
*ORD* = Dataset ordered by a specific variable<br>
*DOM* = Domain of imputation<br>
<h5>*STRAT_LVLIMP_DIS_NN_METH_NOISE* --> for kNN strategy</h5>
*STRAT* = Strategy used<br>
*LVLIMP* = Level of imputation<br>
*NN* = Number of nearest neighbours<br>
*DIS* = Distance variable <br>
*METH* = Methodology of sampling of NNs<br>
*NOISE* = With or Without added random noises for ties<br>

* Definitions:<br>
*movement entry* = From Sub-district ---> To community displacement.
<b>For Data Unit:</b> VIM package from R was used for both Hot Deck and kNN simulations.

### Simulations of Randomized NA

As mentioned in the methodologies section, we used two different strategies to get to the best results. For each strategie, we ran different simulation and tweaked parameters to try compare different outcomes. Here is the results:<br>

<h4><b>Simulations on community level of imputation:</b></h4>

4 approaches, result is the mean of 200 simulation for each approach.

```{r echo=FALSE, warning=FALSE}
library(readxl)
plot1com_draft <- read_excel("output/plot1com_draft.xlsx") %>% 
  rename("Error rate" = Margin_Error)
ggplot(plot1com_draft, aes(x=Approach, y=`Error rate`, label = `Error rate`), width= 50)+
  geom_col(fill = "#ee5859", width = 0.2,position = position_dodge(width=0.1), aes(ylab = `Error rate`)) +
  theme(axis.text.x = element_text(vjust = 0.5,angle = 25)) +
  geom_text(nudge_y = 0.03)
```
<br>
Looking at community aggregation level above, we can conclude that with same number of simulations for all, the <b>hot deck imputation "to" districts as domains</b> ordered by number of IDP household of arrivals was the best out of 3.

<h4><b>Simulations on Household level of imputation:</b></h4>
<b>1. </b><br>
3 approaches, results to test what best number of NNs for KNN strategy is best (50, 100, or 200)
```{r echo=FALSE, warning=FALSE}

plot2com_draft <- read_excel("output/plot2com_draft.xlsx") %>% 
  rename("Error rate" = Margin_Error)
ggplot(plot2com_draft, aes(x=Approach, y=`Error rate`, label = `Error rate`), width= 50)+
  geom_col(fill = "#ee5859", width = 0.2,position = position_dodge(width=0.1), aes(ylab = `Error rate`)) +
  theme(axis.text.x = element_text(vjust = 0.5,angle = 25)) +
  geom_text(nudge_y = 0.03)
```

<br>
For the 3 community approaches, 50 nearest neighbours was the most adequate. So, next simulation will be running on 50 as Number of NNs and other tweaking of few other variables. 
<br>

<b>2. </b><br>

3 approaches, all kNN strategy, 50 NNs but different parameters<br>
```{r echo=FALSE, warning=FALSE}

plot3com_draft <- read_excel("output/plot3com_draft.xlsx") %>% 
  rename("Error rate" = Margin_Error)
ggplot(plot3com_draft, aes(x=Approach, y=`Error rate`, label = `Error rate`), width= 50)+
  geom_col(fill = "#ee5859", width = 0.2,position = position_dodge(width=0.1), aes(ylab = `Error rate`)) +
  theme(axis.text.x = element_text(vjust = 0.5,angle = 25)) +
  geom_text(nudge_y = 0.03)
```
<br>
After running these 3 simulation, we can conclude the following points:<br>
<ul>
<li>1st simulation, we added HH of arrival also to the distance variables (lat and lon). This change increased slightly the accuracy of predictions</li>
<li>2nd simulation, we set up the aggregation of the NNs to max category instead of a proportional sampling. The result gave almost same result as before.</li>
<li>3rd simulation, we removed the random noise generated which broke some ties and the result is also slightly worse.</li>
</ul>

<b>3. </b><br>

3 approaches, all Hot Deck strategy with different order and domain parameters. 

```{r echo=FALSE, warning=FALSE}

plot4com_draft <- read_excel("output/plot4com_draft.xlsx") %>% 
  rename("Error rate" = Margin_Error)
ggplot(plot4com_draft, aes(x=Approach, y=`Error rate`, label = `Error rate`), width= 50)+
  geom_col(fill = "#ee5859", width = 0.2,position = position_dodge(width=0.1), aes(ylab = `Error rate`)) +
  theme(axis.text.x = element_text(vjust = 0.5,angle = 25)) +
  geom_text(nudge_y = 0.03)
```
<br>
Comparing this graph, with the first one generated, we can conclude that the same pattern was drawn. All results was similar to the same results run with the Hot Deck strategy using community level of imputation but with slight better results. <br>

### Simulations of Forced NA

<h4>Map showing communities reporting NA for sub-district of departure in December 2020</h4>

We also checked the geographical distribution of communities reporting *unknown* as Sub-district of departure. It is clear from the below map that the distribution is not random. <br>

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(leaflet)
library(sf)
data <- readxl::read_excel("data/Consolidated_updated.xlsx")

data <- data %>% 
  select(-c(`End of data coverage period`)) %>% 
  mutate(`Start of data coverage peirod` = case_when(
    `Start of data coverage peirod` %in% c("2020-12-01","2020-12-16") ~ "December 2020",
    `Start of data coverage peirod` %in% c("2020-11-01","2020-11-16") ~ "November 2020",
    `Start of data coverage peirod` %in% c("2020-10-01","2020-10-16") ~ "October 2020",
    `Start of data coverage peirod` %in% c("2020-09-01","2020-09-16") ~ "September 2020",
    `Start of data coverage peirod` %in% c("2021-01-01","2021-01-16") ~ "January 2021",
    `Start of data coverage peirod` %in% c("2021-02-01","2021-02-16") ~ "February 2021",
    `Start of data coverage peirod` %in% c("2021-03-01","2021-03-15","2021-03-16") ~ "March 2021",
    `Start of data coverage peirod` %in% c("2021-04-01","2021-04-16") ~ "April 2021",
    `Start of data coverage peirod` %in% c("2021-05-01","2021-05-16") ~ "May 2021",
    `Start of data coverage peirod` %in% c("2021-06-01","2021-06-15","2021-06-16") ~ "June 2021",
    `Start of data coverage peirod` %in% c("2021-07-01","2021-07-15","2021-07-16") ~ "July 2021",
    `Start of data coverage peirod` %in% c("2021-08-01","2021-08-15","2021-08-16","2021-08-19") ~ "August 2021",
    `Start of data coverage peirod` %in% c("2021-09-01","2021-09-15","2021-09-16") ~ "September 2021",
    `Start of data coverage peirod` %in% c("2021-10-01","2021-10-15","2021-10-16") ~ "October 2021",
    `Start of data coverage peirod` %in% c("2021-11-01","2021-11-15","2021-11-16") ~ "November 2021",
    `Start of data coverage peirod` %in% c("2021-12-01","2021-12-16") ~ "December 2021",
    `Start of data coverage peirod` == "2022-01-01" ~ "January 2022"
  )) %>% 
  filter(!str_detect(`Closest p-code`,"CP") & !str_starts(`Closest p-code`,"RE"))

data <- data %>% 
  select(enumID,`Start of data coverage peirod`,`Closest p-code`,`Number of IDP HH arrivals`, `Subdistrict of last departure 1`, `Number of IDP HHs arrived from subdistrict 1`,
         `Subdistrict of last departure 2`, `Number of IDP HHs arrived from subdistrict 2`,
         `Subdistrict of last departure 3`, `Number of IDP HHs arrived from subdistrict 3`) %>% 
  rename("To_Com" = `Closest p-code`,
         "From_Sub"=`Subdistrict of last departure 1`) %>% 
  mutate(`Number of IDP HHs arrived from subdistrict 1` = ifelse(`Number of IDP HHs arrived from subdistrict 1` == "Not sure", NA,`Number of IDP HHs arrived from subdistrict 1`),
         `Number of IDP HHs arrived from subdistrict 2` = ifelse(`Number of IDP HHs arrived from subdistrict 2` == "Not sure", NA,`Number of IDP HHs arrived from subdistrict 2`),
         `Number of IDP HHs arrived from subdistrict 3` = ifelse(`Number of IDP HHs arrived from subdistrict 3` == "Not sure", NA,`Number of IDP HHs arrived from subdistrict 3`)) %>% 
    filter(`Number of IDP HH arrivals` != 0) 
data$`Number of IDP HHs arrived from subdistrict 1` <- as.numeric(data$`Number of IDP HHs arrived from subdistrict 1`)
data$`Number of IDP HHs arrived from subdistrict 2` <- as.numeric(data$`Number of IDP HHs arrived from subdistrict 2`)
data$`Number of IDP HHs arrived from subdistrict 3` <- as.numeric(data$`Number of IDP HHs arrived from subdistrict 3`)

locations <- read.csv("data/ocha_locations.csv")

locations <- locations %>%
  rename(lat=Latitude_y, long=Longitude_x, community_pcode=admin4Pcod) %>% 
  select(lat, long, community_pcode) %>%
  rename(To_Com = community_pcode) %>%
  distinct(To_Com, .keep_all = TRUE)
```

```{r message=FALSE, warning=FALSE, include=FALSE}

gps_na_dec2020 <- data %>% 
  filter(`Start of data coverage peirod` == "December 2020") %>% 
  mutate(clean = ifelse(is.na(`Number of IDP HHs arrived from subdistrict 1`) & is.na(`Number of IDP HHs arrived from subdistrict 3`) & is.na(`Number of IDP HHs arrived from subdistrict 2`),T,
                        ifelse(is.na(`Number of IDP HHs arrived from subdistrict 3`) & is.na(`Number of IDP HHs arrived from subdistrict 2`) &
                          `Number of IDP HH arrivals` != `Number of IDP HHs arrived from subdistrict 1`,T,
                        ifelse(is.na(`Number of IDP HHs arrived from subdistrict 3`) & sum(`Number of IDP HHs arrived from subdistrict 1`,`Number of IDP HHs arrived from subdistrict 2`) != `Number of IDP HH arrivals`,T,F))))
gps_na_dec2020 <- gps_na_dec2020 %>% 
  mutate(clean = ifelse(is.na(clean), FALSE, clean),
         `Number of IDP HHs arrived from subdistrict 1` = ifelse(is.na(`Number of IDP HHs arrived from subdistrict 1`), 0, `Number of IDP HHs arrived from subdistrict 1`),
         `Number of IDP HHs arrived from subdistrict 2` = ifelse(is.na(`Number of IDP HHs arrived from subdistrict 2`), 0, `Number of IDP HHs arrived from subdistrict 2`),
         `Number of IDP HHs arrived from subdistrict 3` = ifelse(is.na(`Number of IDP HHs arrived from subdistrict 3`), 0, `Number of IDP HHs arrived from subdistrict 3`),
         `Subdistrict of last departure 3` = ifelse(clean == T & (`Number of IDP HH arrivals` - (`Number of IDP HHs arrived from subdistrict 1` + `Number of IDP HHs arrived from subdistrict 2`)) == 1, "To Remove", `Subdistrict of last departure 3`),
         percentage = ifelse(clean == T,100*( 1 - ((`Number of IDP HHs arrived from subdistrict 1` + `Number of IDP HHs arrived from subdistrict 2` + `Number of IDP HHs arrived from subdistrict 3`) / `Number of IDP HH arrivals`)),0))

unknown_com <- gps_na_dec2020 %>% 
  select(To_Com,percentage, enumID) %>% 
  filter(percentage != 0) %>% 
  left_join(locations, by = "To_Com")

known_com <- gps_na_dec2020 %>% 
  select(To_Com,percentage) %>% 
  filter(percentage == 0) %>% 
  left_join( locations, by = "To_Com")

unknown_com.sf <- st_as_sf(x = unknown_com, coords = c("long", "lat"), crs = 4326)
known_com.sf <- st_as_sf(x = known_com, coords = c("long", "lat"), crs = 4326)
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
leaflet() %>%
  addCircles(data = known_com.sf, stroke = F, fill= T, color = "black", fillOpacity = 0.75,
             radius = 500) %>% 
  addCircles(data = unknown_com.sf, stroke = F, fill= T, color = "#ee5859",
             fillOpacity = 0.75,
             radius = unknown_com.sf$percentage * 25) %>%
  addProviderTiles("OpenStreetMap.Mapnik")
```

<h4>Forced NA data imputation simulation</h4>
Looking at the map above, we forced NA entries in the *From Sub-district* field to 4 sub-districts of Arrivals were NA are distributed (Dana, Daret Azza, Jandairis, and Raju). Below is the same outputs from the previous simulations on community aggregation levels. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
plot1com_draft_F <- read_excel("output/plot1com_draft_F.xlsx") %>% 
  rename("Error rate" = Margin_Error)
ggplot(plot1com_draft_F, aes(x=Approach, y=`Error rate`, label = `Error rate`), width= 50)+
  geom_col(fill = "#ee5859", width = 0.2,position = position_dodge(width=0.1), aes(ylab = `Error rate`)) +
  theme(axis.text.x = element_text(vjust = 0.5,angle = 25)) +
  geom_text(nudge_y = 0.03)

plot2com_draft_F <- read_excel("output/plot2com_draft_F.xlsx") %>% 
  rename("Error rate" = Margin_Error)
ggplot(plot2com_draft_F, aes(x=Approach, y=`Error rate`, label = `Error rate`), width= 50)+
  geom_col(fill = "#ee5859", width = 0.2,position = position_dodge(width=0.1), aes(ylab = `Error rate`)) +
  theme(axis.text.x = element_text(vjust = 0.5,angle = 25)) +
  geom_text(nudge_y = 0.03)

plot3com_draft_F <- read_excel("output/plot3com_draft_F.xlsx") %>% 
  rename("Error rate" = Margin_Error)
ggplot(plot3com_draft_F, aes(x=Approach, y=`Error rate`, label = `Error rate`), width= 50)+
  geom_col(fill = "#ee5859", width = 0.2,position = position_dodge(width=0.1), aes(ylab = `Error rate`)) +
  theme(axis.text.x = element_text(vjust = 0.5,angle = 25)) +
  geom_text(nudge_y = 0.03)

plot4com_draft_F <- read_excel("output/plot4com_draft_F.xlsx") %>% 
  rename("Error rate" = Margin_Error)
ggplot(plot4com_draft_F, aes(x=Approach, y=`Error rate`, label = `Error rate`), width= 50)+
  geom_col(fill = "#ee5859", width = 0.2,position = position_dodge(width=0.1), aes(ylab = `Error rate`)) +
  theme(axis.text.x = element_text(vjust = 0.5,angle = 25)) +
  geom_text(nudge_y = 0.03)
```
 
```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
inputPanel(
  selectInput("date", label = "Select Month", choices = unique(data$`Start of data coverage peirod`), selected = "January 2022")
)
```

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
library(leaflet)
filteredData <- reactive({
  req(input$date)
  data[data$`Start of data coverage peirod` == input$date,]
})

observe({
if(nrow(data) > 0){
   monthly_data <- filteredData() %>%
     mutate(clean = ifelse(is.na(`Number of IDP HHs arrived from subdistrict 1`) & is.na(`Number of IDP HHs arrived from subdistrict 3`) & is.na(`Number of IDP HHs arrived from subdistrict 2`),T,
                           ifelse(is.na(`Number of IDP HHs arrived from subdistrict 3`) & is.na(`Number of IDP HHs arrived from subdistrict 2`) &
                          `Number of IDP HH arrivals` != `Number of IDP HHs arrived from subdistrict 1`,T,
                          ifelse(is.na(`Number of IDP HHs arrived from subdistrict 3`) & sum(`Number of IDP HHs arrived from subdistrict 1`,`Number of IDP HHs arrived from subdistrict 2`) != `Number of IDP HH arrivals`,T,F))))
   
monthly_data <- monthly_data %>% 
  mutate(clean = ifelse(is.na(clean), FALSE, clean),
         `Number of IDP HHs arrived from subdistrict 1` = ifelse(is.na(`Number of IDP HHs arrived from subdistrict 1`), 0, `Number of IDP HHs arrived from subdistrict 1`),
         `Number of IDP HHs arrived from subdistrict 2` = ifelse(is.na(`Number of IDP HHs arrived from subdistrict 2`), 0, `Number of IDP HHs arrived from subdistrict 2`),
         `Number of IDP HHs arrived from subdistrict 3` = ifelse(is.na(`Number of IDP HHs arrived from subdistrict 3`), 0, `Number of IDP HHs arrived from subdistrict 3`),
         `Subdistrict of last departure 3` = ifelse(clean == T & (`Number of IDP HH arrivals` - (`Number of IDP HHs arrived from subdistrict 1` + `Number of IDP HHs arrived from subdistrict 2`)) == 1, "To Remove", `Subdistrict of last departure 3`),
         percentage = ifelse(clean == T,100*( 1 - ((`Number of IDP HHs arrived from subdistrict 1` + `Number of IDP HHs arrived from subdistrict 2` + `Number of IDP HHs arrived from subdistrict 3`) / `Number of IDP HH arrivals`)),0))

  unknown_com <- monthly_data %>% 
    select(To_Com,percentage, enumID) %>% 
    filter(percentage != 0) %>% 
    left_join(locations, by = "To_Com")
  
  known_com <- monthly_data %>% 
    select(To_Com,percentage) %>% 
    filter(percentage == 0) %>% 
    left_join( locations, by = "To_Com")
  unknown_com.sf <- st_as_sf(x = unknown_com, coords = c("long", "lat"), crs = 4326)
  known_com.sf <- st_as_sf(x = known_com, coords = c("long", "lat"), crs = 4326)
  output$map <- renderLeaflet({
   leaflet(options = leafletOptions(zoomControl = FALSE)) %>%  
      addCircles(data = known_com.sf, stroke = F, fill= T, color = "black", fillOpacity = 0.75,
                 radius = 500) %>% 
      addCircles(data = unknown_com.sf, stroke = F, fill= T, color = "#ee5859",label = unknown_com.sf$enumID, 
                 labelOptions = labelOptions(noHide = T), fillOpacity = 0.75,
             radius = unknown_com.sf$percentage * 30) %>%
      addProviderTiles("OpenStreetMap.Mapnik")
  }) 
}
 
})
leafletOutput("map", width = "100%",height = "700px")
```



### Reflection

<b><h4>Reflection:</h4></b>

<ul>
<li>Setting up of NAs should replicate the most realistic scenarios that might occur.</li>
<li>Trying to figure out how the kNN is calculating the distances of closest neighboring communities and find out how the distribution of samples are taken.</li>
<li>Follow up for number 2, check the distribution of Sub-districts of departures for the closest communities using the Euclidean metric and see if it is aligning with the kNN distribution.</li>
<li>Following up with C&D Unit (Sarah Vassalo) to try to fix tool limitations.</li>
</ul>




